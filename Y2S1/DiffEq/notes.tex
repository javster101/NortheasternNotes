\documentclass{article}
\usepackage[utf8]{inputenc}
\title{Differential Equations: MATH2341}
\author{Javier Coindreau}

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{example}{Example}
\newtheorem{derivation}{Derivation}

\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{mathtools}
\usepackage{mathrsfs,relsize,array}

\newcommand\laplace{\mathlarger{\mathlarger{\mathscr{L}}}}

\begin{document}

\begin{titlepage}
	\maketitle
	\tableofcontents{}
\end{titlepage}


\section{Chapter 2}

\subsection{2nd Order Differential Equations}
A 2nd order differential equation is an equation containing a 2nd order derivative of form $a_2 x y'' + a_1 x y' + a_0 x y = f(x)$, where $a_2, a_1, a_0, f(x)$ are all continuous functions.
If $f(x) \neq 0$, then it is a non-homogeneous equation, otherwise it is homogeneous.

For our purposes, $a_2, a_1, a_0$ will all be nonzero constants, allowing for easier solutions.
\subsubsection{Verifying DE \& Solutions}
How many solutions do we expect for $ay''+by'+cy=0$?


\begin{example} $y''-y'-6y=0$
	\begin{enumerate}
		\item Verify that $y_1x=e^{3x}$
			\\ Plug in: $y_1=e^{3x}, y_2=3e{3x}, y_3=9e{3x}$
			\\ $9e^{3x}-3e^{3x}-6e^{3x}=0$
			\\ Valid solution
		\item Verify that $y_1=2e^{3x}$
			\\ Plug in $y_1=2e^{3x}, y_2=6e{3x}, y_3=18e{3x}$
			\\ $18e^{3x}-6e^{3x}-12e^{3x}=0$
			\\ Valid solution
		\item Verify that $y_1=2e^{-2x}$
			\\Plug in $y_1=e^{-2x}, y_2=-2e{-2x}, y_3=4e{-2x}$
			\\ $4e^{3x}+2e^{3x}-6e^{3x}=0$
			\\ Valid solution
	\end{enumerate}
\end{example}

Any second order differential equation has at most two linearly independent solutions.
\begin{definition}
	Two functions $y_1$ and $y_2$ are linearly independent $\iff y_1 \neq ky_2, k \in \mathbb{R}$
\end{definition}

\subsubsection{Solutions \& Superposition}
Suppose that $y_1, y_2$ are two linearly independent functions that satisfy a linear homogeneous differential equation. For such solutions, a general solution to the differential equation is 
\begin{equation} y(x)=C_1 y_1(x)+C_2 y_2(x); C_1,C_2 \in \mathbb{R} \end{equation}

\begin{itemize}
	\item $e^{r_1 x},e^{r_2 x}$ are independent if $r_1 \neq r_2 $
	\item $\sin{px}, \sin{qx}$ are independent if $p \neq q$
\end{itemize}

\begin{example} $y''-y'-6y=0$
	\\ In this case, the general solution is $y(x)=C_1e^{3x}+C_2e^{-2x}$.
\end{example}

\subsection{2.3: Solving 2nd order homogeneous equations with constant coefficients}

	What function has a $y, y', y''$ that are just scalar multiples of $y$?
	
	$e^{rx}$ always works, as $y=e^{rx}, y'=re^{rx}, y''=r^2e^{rx}$. When plugged in, this results in $ar^2e^{rx}+bre^{rx}+ce^{rx}=0$. $e^{rx}(ar^2+br+c)=0$. As the left term never becomes 0, the right term must.
	
Solve: $ar^2+br+c=0$
\begin{indent}
\\	This is known as the characteristic equation, as it resembles the original.
\end{indent}
\\Solutions can be derived from the quadratic function: $r_1, r_2=\frac{-b +- \sqrt{b^2-4ac}}{2a}$
	\\From this, we find $r_1, r_2$ and get the two solutions, $e^{r_1 x}, e^{r_2 x}$
\\3 possiblities arise from this.
\begin{enumerate}
	\item $b^2>4ac$.
		\\$r_1=\frac{-b + \sqrt{b^2-4ac}}{2a}, r_2=\frac{-b -\sqrt{b^2-4ac}}{2a}$
	\item $b^2=4ac$ (Be careful!)
		\\$r_1=r_2=\frac{-b}{2a}$. This cannot happen, both are linearly dependent and we need two linearly independent solution! If $y_1=e^{r_1 x}$, how can we get a linearly independent function $y^2$ that satisfies the same equation?
		\\Trick is to multiply $y_1$ with an extra $x$, resulting in $xe^{r_1 x}$. This works due to Abel's Theorem.
		\begin{theorem}
		If $y_1(x)$ is a solution to the equation $y''+p(x)y'+q(x)y=0$, then the second linearly independent solution is given by $y_2(x)=y_1(x)*\int{\frac{e^{-\int{p(x)dx}}}{(y_1(x))^2}dx}.$\end{theorem}
		Our equation, in standard form, becomes $y''+\frac{b}{a}y'+\frac{c}{a}y'=0$.
	$p(x)$ becomes $\frac{b}{a}$, and evaluating the integral results in $xe^{r_1}x$.
	\\The general solution results in $y(x)=C_1 e^{r_1 x}+C_2 xe^{r_1 x}$
	\item $b^2<4ac$
		\\$\frac{-b + \sqrt{b^2-4ac}}{2a}=\frac{-b \pm i\sqrt{b^2-4ac}}{2a}, 
		\frac{-b}{2a}\pm i \frac{\sqrt{4ac-b^2}}{2a}$.
		\\We want to remove $i$. 
		To do so, we simplify this to $p+iq$, where $p,q \in \mathbb{R}$. 
		We want to find two real valued independent functions.
		
		$e^{r_1 x}=e^{(p+iq)x}=e^{px}e^{iqx}, e^{r_2x}=e^{(p-iq)x}=e^{px}e^{-iqx}$.
		To solve this, we use Euler's Identity:
		\begin{derivation}
			\begin{equation}
				e^{i\theta}=\cos{\theta}+i \sin{\theta}
			\end{equation}
			$e^\theta=1+\theta+\frac{\theta^2}{2!}..., 
			\cos{\theta}=1-\frac{\theta^2}{2!}...,
			\sin{\theta}=\theta-..., 
			\\e^{i\theta}=\cos{\theta}+i \sin{\theta}$ 
			due to presence of imaginary constant.
		\end{derivation}
		We can use this to extract the $i$ from the exponent: 
		$e^{px}e^{iqx}=e^{px}(\cos{px}+i\sin{qx}, e^{px}e^{iqx}=e^{px}(\cos{px}-i\sin{qx}, 
		\\y_1(x)=e^{px}\cos{qx}, y_2(x)=e^{px}\sin{qx}$.
		
		The general solution becomes 
		\begin{equation}
			y(x)=C_1e^{px}\cos{qx}+C_2e^{px}\sin{qx}=e^{px}(C_1\cos{qx}+C_2\sin{qx}
		\end{equation}
		Note: if $p=0, y(x)=C_1\cos{qx}+C_2\sin{qx}$ and becomes an oscillatory motion. 
		If $p>0$, oscillations get stronger over time, and if $p<0$ motion becomes dampened.
\end{enumerate}

\begin{example} $y''-y'-6y=0$. \\The characteristic equation is $r^2-r-6=0$, which is factorable here to $(r-3)(r+2)=0, r_1=3, r_2=-2$. From this, the general solution becomes $y(x)=C_1e^{3x}+C_2e^{-2x}$.
\end{example}
\begin{example} $y''+6y'+9y=0$
	\\The characteristic equation is $r^2+6r+9=0$ $6^2=4*1*9, 36=36$, root at $-3$. 
	The general solution becomes $y(x)=C_1 e^{-3x}+C_2xe^{-3x}$
\end{example}
\begin{example}
	$y''+16y=0$
	\\The characteristic equation is $r^2+16=0, r^2=-16, r=\pm\sqrt{-16}=0\pm4i$.
	The general solition becomes $y(x)=C_1\cos{4x}+C_2\sin{4x}$
\end{example}
\begin{example}
	Find a particular solution for 
	\begin{enumerate}
		\item $y''+4y'+29y=0$
			\\Not factorable, quadratic equation results in
			$\frac{-4}{2} \pm \frac{10i}{2}$
			\\$p=-2, q=5i, y(x)=e^{-2x}[C_1 \cos{5x} + C_2 \sin{5x}]$
			\\ Solve when $y(0)=1 
			\\y'(0)=1$: $y(0)=e^1 [C_1 \cos{0} + C_2 \sin{0} = 1*(C_1*1), C_1=1$
			\\ Solve when $y'(0)=1$
			\\$y'(x)=-2e^{-2x}[C_1 \cos{5x} + C_2 \sin{5x}]+(-5 C_1 \sin{5x}+5 C_2 \cos{5x})$
			\\$y'(0)=-2[C_1*1+C_2*0]+(-5 C_1 * 0 + 5 C_2 * 1) = -2C_1+5C_2, C_2=\frac{2}{5}$
			\\The particular solution becomes $y_p(x)=e^{-2x}[\cos{5x}+\frac{2}{5}\sin{5x}]$
		\item $y''+3y'+2y=0$
			\\Factors to $(r+2)(r+1), r_1=-2, r_2=-1, y(x)=C_1e^{-2x}+C_2e^{-1x}$
			\\Solve for $y(0)=1$
			\\$y(0)=C_1+C_2=1$
			\\Solve for $y'(0)=1$
			\\$y'(x)=-2 C_1 e^{-2x}-C_2e^{-1x}=-2 C_1- C_2=1$
			\\Solving for $C_1$ gives $C_1=-2, C_2=3$
			\\The particular solution becomes $y(x)=-2e^{-2x}+3e^{-x}$
		\item $9''+6y'+9y=0$
			\\Factors to one root at $-3, y(x)=C_1e^{-3x}+C_2xe^{-3x}$
			\\Solve for $y(0)=2$: $y(0)=C_1=2$
			\\Solve for $y'(0)=3$
			\\$y'(x)=-3C_1e^{-3x}+C_2e^{-3x}-3C_2e^{-3x}$
			\\$y'(0)==3C_1+C_2-3C_2=3*2-2C_2, C_2=9$
			\\Particular solution becomes $y(x)=2e^{-3x}+yxe^{-3x}$
	\end{enumerate}
\end{example}
\subsection{2.4: Applications}
\subsubsection{Free Mechanical Vibrations}
Given a spring attached to a mass with a dampener, what mathematical model can simulate its motion? Since it is a free vibration, it has no external forces. This means that it is guided purely by the force for the spring and the dampener.
Since we are observing accelerated motion, it will be a 2nd order differential equation ($a=\frac{d^2x}{dt^2}$).
The total force from this spring is given by Hooke's law, 
\begin{equation} F_s = -kx, k=\frac{|F_s|}{m} > 0 \end{equation}
The damping force exerted on the system by the dashpot is proportional to its velocity,
\begin{equation} F_d = -cv \end{equation}
where $c$ is the damping constant.

We know that the velocity $v=\frac{dx}{dt}$, so the total force becomes $F=F_s+F_d$, and replacing fixed values with differentials becomes $m \frac{d^2x}{dt^2} = -kx -c \frac{dx}{dt}$, or alternatively $mx''+cx'+kx=0$.

Being a real world problem, we know that $m$ must be greater than 0, $c$ may be 0 if there is no dashpot, and $k$ is greater than 0 due to the presence of the spring.
At equilibrium, $x(0)=0, x'(0)=0$ as the object starts at rest. In this case, we can take compression as movement in the negative $x$ axis and stretching in the positive $x$ axis.

Now, let's go through different cases.
\begin{enumerate}[Opt. a: ]
	\item No dashpot (undamped oscillation, $c=0$):
		\\Since it is undampened, this oscillation should never end. 
		The equation in this case would be $mx''+kx=0$, simplifying to $x''+\frac{k}{m}x=0$. 
		We can then extract the constant $\omega$, the natural frequency of the system, as $\omega=\sqrt{\frac{k}{m}}, \omega^2=\frac{k}{m}$.
		\\The characteristic equation here becomes 
		$r^2+\omega^2=0, r^2=-\omega^2, r=\pm i\omega$.
		This equation will consist of purely oscillatory terms, and will become 
		$x(t)=C_1 \cos{\omega t}+C_2 \sin{\omega t}$.
		It can be confusing for it to have both sin and cos, so we can rewrite this in amplitude-phase form, resulting in $x(t)=A \cos{(\omega t-\phi)}$, where $\phi=\tan^{-1}{C_2/C_1}$.
	\begin{derivation}
		\begin{equation} x(t)=A \cos{(\omega t-\phi)} \end{equation}
		From original, we create $\sqrt{C_1^2+C_2^2}(\frac{C_1 \cos{\omega t}}{\sqrt{C_1^2+C_2^2}}+\frac{C_1 \sin{\omega t}}{\sqrt{C_1^2+C_2^2}})$. 
		We then extract $A$, which becomes the length of a triangle with sides $C_1,C_2$, from the Pythagorean Theorem, creating a new function $A(\frac{\cos {\omega t}}{A} + \frac{\sin {\omega t}}{A})$. We can then use the trig identity $\cos{(A-B)}=\cos{A}\cos{B}+\sin{A}\sin{B}$ to simplify and create the final function.
	\end{derivation}
	\item Damped oscillation ($c \neq 0$)
		\\The equation is $mx''+cx'+kx=0$, with a characteristic equation of $mr^2+cr+k=0$. 
		We can use the quadratic formula here to get the roots as $\frac{-c \pm \sqrt{c^2-4mk}}{2m}$,
		resulting in roots of $r_1=\frac{-c}{2m}+\frac{\sqrt{c^2-4mk}}{2m}, r_2=\frac{-c}{2m}-\frac{\sqrt{c^2-4mk}}{2m}$. 
				Since they are both negative due to the negative poking outside, we can guarantee that $r_2$ is negative since $m,k,c>0$.
				$r_1$ is also negative, since $\sqrt{x^2-b}$ will always be less than just $x$ if $b$ is positive, which it is in our case. We can then use the cases from the first section.
		\begin{enumerate}
			\item $C^2>4mk$ is considered overdampened, as there will be two real negative roots. This results in $x(t)=C_1e{r_1t}+C_2e^{r_2t}$
			\item $C^2=4mk$ results in a critically damped solution with one real root. This results in $x(t)=C_1e^{r_1t}+C_2xe^{r_1t}=e^{r_1t}(C_1+C_2x)$
			\item $C^2<4mk$ is underdampened, so oscillations will appear. 
				In this case we get $x(t)=e^{\frac{-C}{2m}}[C_1\cos{(\frac{\sqrt{4mk-C^2}}{2m})}+C_2\sin{(\frac{\sqrt{4mk-C^2}}{2m})}$. 
				This function has no natural frequency, and $q$ in this case is considered a pseudofrequency due to damping.
			This can be rewritten using amplitude-phase as $x(t)=Ae^{\frac{-C}{2m}}\cos{(\omega t-\phi)}$
		\end{enumerate}
\end{enumerate}

\subsection{Non homogeneous 2nd order differential equations}
These are of the form $ay''+by'+cy=f(x)$, where $f(x)$ is a differentiable and continuous function.
\begin{theorem}
	Given a differential equation of the form $ay''+by'+cy=f(x)$, the general solution is given by 
	\begin{equation} y(x)=y_h(x)+y_p(x) \end{equation} where $y_h(x)$ is the homogeneous solution to the
	differential equation and $y_p(x)$ is a particular solution for the non-homogeneous equation.
\end{theorem}

\subsubsection{Finding $y_p(x)$ with undetermined coefficients}

To find $y_p(x)$, we first make a guess for $y_p$ based on $f(x)$ with an unknown coefficient. This is then plugged into the differential equation  $ay_p''+by_p'+cy_p=f(x)$ and solved for the unknown coefficient.

The coefficient is chosed based on the behavior of $f(x)$

\begin{enumerate}
	\item $f(x)=\alpha e^{kx}$, the guess should be $y_p=Ae^{kx}$
	\item $f(x)=\sin{kx}$ or $\cos{kx}$. This is a difficult case, because unlike the above, the canceling step in the examples does not automatically happen.
		Therefore, we use $y_p=A\cos{(kx)}+B\sin{(kx)}$. It is very important to write both, even if only one is actually there.
	\item $f(x)=$polynomial of degree $n$, guess should be a polynomial of the same degree.
		Being a general polynomial, all terms must be written even if some terms are missing. Examples are $f(x)=2x, y_p=Ax+B$
\end{enumerate}

\begin{example}
	$y''+3y'+2y=2e^{4x}$, find the general solution.
	
	 \begin{enumerate}[Step 1:]
		 \item Always find the homogeneous solution: $y''+3y'+2y=0, r^2+3r+2=0, (r-1)(r-2)=0, r_1=1, r_2=2, y_h(x)=C_1e^x+C_2e^{2x}$
	
		 \item Find the specific solution: Guess $y_p=Ae^{4x}$. 
	\\Then, find $y', y''; y'=4Ae^{4x}, y''=16Ae^{4x}$. 
	\\Plugged in, $16Ae^{4x}-12Ae^{4x}+2Ae^{4x}=2e^{4x}$. 
	\\We can cancel $e^{4x}$, resulting in $16A-12A+2A=2, 6A-2, A=1/3$. 
	\\Therefore, $y_p=\frac{1}{3}e^{4x}$

		\item Combine both to create $y(x)=C_1e^x+C_2e^{2x}+\frac{1}{3}e^{4x}k$
	\end{enumerate}
\end{example}

\begin{example}
	$y''+3y'+2y=2sin{3x}$
	\begin{enumerate}[Step 1:]
		\item Homogeneous solution is $y_h(x)=C_1e^x+C_2e^{2x}$, same as before.

		\item Guess: $y_p=A\cos{3x}+B\sin{3x}$.
	\\$y', y'': y'=-3A\sin{3x}+3B\cos{3x}, y''=-9A\cos{3x}-9B\sin{3x}$
	\\Plugged in, 
	\\$-9A\cos{3x}-9B\sin{3x}
	\\-9B\cos{3x}+9A\sin{3x}
	\\+2A\cos{3x}+2B\sin{3x}
	\\=0\cos{3x}+2\sin{3x}$
	\\After simplifying, $(-7A-9B)\cos{3x}+(9A-7B)\cos{3x}=0\cos{3x}+2\sin{3x}$.
	Terms should match, so $9A-7B=2, -7A+9B=0, B=\frac{-7}{65}, A=\frac{9}{64}$

		\item Merge, $y(x)=C_1e^x+C_2e^{2x}+\frac{9}{65}\cos{3x}-\frac{7}{65}\sin{3x}$
	\end{enumerate}
\end{example}

\begin{example}
	$y''+3y'+2y=2x+1$
	\begin{enumerate}[Step 1:]
		\item $y_h(x)=C_1e^x+C_2e^{2x}$

		\item Guess: $y_p=Ax+B, y_p'=A, y_p''=0$
			\\Plugged in, $-3A+2Ax+2B=2x+1, A=1, B=2, y_p=x+2$
		\item Merge, $y(x)=C_1e^x+C_2e^{2x}+x+2$
	\end{enumerate}
\end{example}

What if multiple of these were combined into the same equation? What if you had $y''-3y'+2y=2e^{4x}+2\sin{3x}+2x+1$?

Fortunately, you can solve them individually and sum them together. With the previous one, for example, the result would be $y(x)=C_1e^{2x}+C_2e^{2x}-1/3 e^{4x}-\frac{7}{65}\cos{3x}+\frac{9}{65}\sin{3x}+x+2$.

Another option would be to start with a single guess containing all the equation types given, solving $y_p = Ae^{4x}+B\cos{3x}+C\sin{3x}+Dx+E$

\begin{example}
	Find the general solution for $y''-y'-2y=3e^{2x}$.

	Characteristic eq: $r^2-r-2=0, (r-2)(r+1)=0, y_h(x)=C_1e^{2x}+C_2e^{-x}$

	Guess: $y_p=Ae^{2x}, y_p'=2Ae^{2x}, y_p''=4Ae^{2x}$. 

	Eq: $4Ae^{2x}-2Ae^{2x}-2Ae^{2x}=3e^{2x}, 0=3e^{2x}$. Impossible? Options are $0=3, e^{2x}=0$. Neither are possible! Therefore, guess is wrong here.
\end{example}

Why is this not possible? Because our guess is linearly dependent with $C_1e^{2x}$, which is not allowed! Since $C_1e^{2x}$ is unchangeable, we must change the guess in the same way we'd do it before, by making the guess $Axe^{2x}$. Unfortunately, solving this brings in the product rule: $y_p'=Ae^{2x}+2Axe^{2x}, y_p''=2Ae^{2x}+2Ae^{2x}+4Axe^{2x}=4Ae^{2x}+4Axe^{2x}$. This is then used as before.

\begin{example}[10]
	$4Ae^{2x}+4Axe^{2x}-Ae^{2x}-2Axe^{2x}-2Axe^{2x}=3e^{2x}, 4Ae^{2x}-Ae^{2x}=3e^{2x}$ (Note: the terms with x must cancel, otherwise math was done wrong)

	$3Ae^{2x}=3e^{2x}, A=1, y(x)=C_1e^{2x}+C_2e^{-x}+xe^{2x}$ (Don't forget the x!)
\end{example}

Using this, we can make a better order of approach:

\begin{enumerate}
	\item Find $y_h$
	\item Write the normal $y_p$ guess
	\item Compare each term in $y_p$ to each term in $y_h$
	\item If any are linearly dependent, multiply ONLY the linearly dependent terms in $y_p$ by $x^n$ where $n$ is the smallest positive integer that ensures that the term is linearly independent.
	\item Solve for $y_p$
\end{enumerate}

\begin{example}
	Find the general solution for $y''-4y'+4y=3e^{2x}$

	$y_h$: $r^2-4r+4=0, (r-2)(r-2), y_h(x)=C_1e^{2x}+C_2xe^{2x}$
	
	$y_p$: $Ae^{2x}$: LINEARLY DEPENDENT. Lowest power of $x$ needed here is $x^2, y_p=Ax^2e^{2x}$

	Differentiate: $y_p'=2Ax^2e^{2x}+2Axe^{2x}, y''=4Axe^{2x}+4Ax^2e^{2x}+4Axe^{2x}+2Ae^{2x}$

	$y_p(x)=4Ax^2e^{2x}+8Axe^{2x}+2Ae^{2x}-8Ax^2e^{2x}-8Axe^{2x}+4Ax^2e^{2x}=2Ae^{2x}=3e^{2x}, 2A=3, A=1.5$

	$y(x)=C_1e^{2x}+C_2xe^{2x}+1.5x^2e^{2x}$
\end{example}

\subsection{Resonance}
What is an example of a resonating system?

\begin{itemize}
	\item Musical Instruments
	\item Noise cancelling headphones
	\item Kids on a swing
	\item Microwave ovens with water
\end{itemize}

In these systems, there is a natural frequency, a frequency at which the system will keep oscillating even when forced to go further.
For example, in a dashpot system $mx''+kx=F_0 \cos{\alpha t}$, resonance happens when the desired frequency $\alpha$ is equal to the ideal frequency $\sqrt{\frac{k}{m}}$

\section{Chapter 3: Laplace Transforms}
\subsection{Background}

Frequency is the inverse of time, in cycles/unit of time. We can take a differential equation in the time domain and transform it into an
algebraic equation in the frequency domain. This makes them notably easier to manipulate when compared to differentials, but
requires us to solve it in this new frequency domain. Once done, we can take it back to the time domain.

\begin{definition}
	Lef $f(t)$ be a function which is not exponentially growing. We define the laplace transform of $f(t)=\laplace[f(t)]=\int_0^\infty{e^{-st}f(t)dt}=F(s)$
\end{definition}

Why does this work?

\begin{itemize}
	\item he negative in the exponential is purely for convergence reasons.
	\item All applications of exponential growth/decay or oscillatory function are analyzable through a laplace transform, partially due
		to Euler's identity $e^{it}=\cos{t}+i\sin{dt}$
\end{itemize}

In Laplace transforms, $e^{st}=e^{at}+e^{ibt}, s=a+ib$. Using this, exponential/sinusoidal/both types of problems are analyzable.

\begin{example}
	$f(t)=k, \laplace[f(t)]=\int_0^\infty{e^{-st} k dt}=k \lim_{b\rightarrow\infty}{\int_0^\infty{e^{-st}dt}}=\frac{k}{s}$
\end{example}

\begin{example}
	$Find \laplace[e^{at}], \int_0^\infty{e^{-st}e^{at}dt}=\lim_{b\rightarrow\infty}{\int_0^b{e^{-(s-a)t}dt}}
	\\=\frac{1}{-s+a}e^{-s+a}, 0\rightarrow \infty, \frac{1}{-s+a}e^{-\infty+a}-1=\frac{1}{s-a}$

\end{example}

Doing this for every Laplace sucks, but luckily all Laplace transforms are tabulated (Available in Canvas->Modules->Handouts(?)).


\begin{example}
	Using tables:

	\begin{itemize}
		\item $\laplace[\sin{3t}]=\frac{3}{s^2+9}$
		\item $\laplace[\cos{2t}]=\frac{s}{s^2+4}$
		\item $\laplace[e^{-3t}\cos{5t}]=\frac{s+2}{(s+2)^2+25}$
	\end{itemize}
\end{example}

Generalizing, $\laplace[a f(t) \pm b g(t)]=a F(s)\pm b G(s)]$

\begin{example}
	$\laplace[2-6e^{3t}+t^4-5\cos{t}]=\frac{2}{s}-\frac{6}{s-3}+\frac{4!}{s^{5}}-5\frac{s}{s^2+1}$
\end{example}

\subsection{Inverse Laplace Transforms}

\begin{definition} 
	Given a function $f(t)$ who's Laplace Transform is $F(s)$, the inverse Laplace Transform is defined by $\laplace^{-1}[F(s)]=f(t)$
\end{definition}

This is a more algebraically involved procedure, as we need to transform cases to match the table.

\begin{example}

	\begin{enumerate}
		\item $\laplace^{-1}[\frac{12}{x}]=12$
		\item $\laplace^{-1}[\frac{1}{s-4}]=e^{4t}$
		\item $\laplace^{-1}[\frac{s}{s^2+16}]=\cos{4t}$
		\item $\laplace^{-1}[\frac{1}{s^2+16}]=?$\\
			This almost looks like case 6, $\frac{k}{s^2+k^2}$. What we can do is $\frac{1}{4}\laplace^{-1}[\frac{4}{s^2+16}]$ which becomes $0.25\sin{4t}$
		\item $\laplace^{-1}[\frac{s}{(s-2)^2+9}]$ resembles 7.
			\\Transform it to $\frac{s-2+2}{(s-2)^2+9}$, then $\frac{s-2}{(s-2)^2+9}+\frac{2}{(s-2)^2+9}$.\\
			Finally use table (transforming $\frac{2}{3}\frac{3}{(s-2)^2+9}$) to become $e^{2t}\cos{3t}+\frac{2}{3}e^{2t}\sin{3t}$
		\item $\laplace^{-1}[\frac{5}{s^2+3s+2}]$\\
			Factor: $\frac{5}{(s+2)(s+1)}$, decompose with partial fractions: $\frac{A}{s+2}+\frac{B}{s+1}, A(s+1)+B(s+2)=5.$\\
			$B(-1+2)=5, B=5, A(-2+1)=5, A=-5, \frac{-5}{s+2}+\frac{5}{s+1}$\\
			Now that its usable, use table: $-5e^{2t}+5e^{-t}$
	\end{enumerate}
\end{example}

\subsection{Initial value problems with Laplace Transforms}

If $y$ is a function of $t$, $\laplace[y(t)]=Y(s)$. What is the Laplace transform for its derivative, $y'$? From the definition, $\int_0^\infty e^{-st}y'(t)dt$, we can do 
integration by parts to get $-y(0)+s\laplace[y(t)]$, \begin{equation}\laplace[y']=sY(s)-y(0)\end{equation} (case 14).

Therefore, the steps involved are:
\begin{enumerate}
	\item Take the Laplace on both sides, and apply the initial condition right away.
	\item Solve algebraically for $Y(s)$.
	\item Find the inverse Laplace transform $y(t)$ of $\laplace^{-1}[Y(s)]$
\end{enumerate}

\begin{example}
	Solve using LT: $y'-3y=e^{5t}, y(0)=-1$\\
	Take Laplace of all terms: $\laplace[y']-3\laplace[y]=\laplace[e^{5t}], sY(s)-y(0)-3Y(s)=\frac{1}{s-5}$\\
	Plug in initial value: $sY(s)+1-3Y(s)=\frac{1}{s-5}$\\
	Algebra: $(s-3)Y(s)=\frac{1}{s-5}-1, Y(s)=\frac{1}{(s-5)(s-3)}-\frac{1}{s-3}$\\
	Find inverse of all terms: $y(t)=\laplace^{-1}[\frac{1}{(s-5)(s-3)}]-\laplace^{-1}[\frac{1}{s-3}]$\\
	Apply partial fractions to first: $1=A(s-3)+B(s-5), B=-0.5, A=0.5, \laplace^{-1}[\frac{0.5}{(s-5)}-\frac{0.5}{(s-3)}+[\frac{1}{s-3}]$\\
	All case 3, apply: $y(t)=0.5e^{5t}-1.5e^{3t}$
\end{example}

This itself is not too useful, but it gets easier and easier with higher order equations. For example, 2nd order equations have a Laplace transform $\laplace[y'']$
of \begin{equation} \laplace[y'']=s^2Y(s)-sy(0)-y'(0) \end{equation}


\begin{example} 
	Solve using LT: $y''+3y'+2y=0, y(0)=1, y'(0)=1$\\
	Take LT of all terms: $(s^2Y(s)-sy(0)-y'(0))+3(sY(s)-y(0))+2Y(s)=0, 
	\\s^2Y(s)-sy(0)-y'(0)+3Y(s)-3y(0)+2Y(s)=0$\\
	Plug in initial values: $s^2Y(s)-s-1+3sY(s)-3+2Y(s)=0, (s^2+3s+2)Y(s)-s-4=0$\\
	Solve for $Y(s)$: $Y(s)=(s+4)(s^2+3s+2)$ (Note, not a coincidence that denominator looks like characteristic eq.)\\
	Factor: $Y(s)=\frac{s+4}{(s+1)(s+2)}$\\
	Take Laplace (with partial frac): $s+4=A(s+2)+B(s+1), s=-2, 2=-B,B=-2, s=-1, A=1, A=3$.\\
	$Y(s)=\laplace^{-1}[\frac{3}{s+1}-\frac{2}{s+2}]$\\
	Find in chart: $Y(s)=3e^{-t}-2e^{-2t}$ (BONUS: We know the form is $C_1e^{rt}+C_2e^{ut}$ from previous chapters, so we know our answer here is correct as it matches!)
\end{example}

In general, a 2nd order equation $ay''+by'+cy=f(t)$ becomes $a(s^2Y(s)-sy(0)-y'(0))+b(sY(s)-y(0))+cY(s)=\laplace[f(t)]$.\\
Rewritten, it becomes $Y(s)=\frac{\laplace[f(t)]+asy(0)+ay'(0)+by(0)}{as^2+bs+c}$, at which point you apply partial fractions if needed.

\subsection{Shifting Theorems and Discontinuous Inputs}


\begin{theorem}[First Shifting Theorem]
	Let $\laplace[f(t)]=F(t)$.

	Then, $\laplace[e^{at}f(t)]=F(s-a)$
\end{theorem}

This is visible in case 2 in the table, $\laplace{t^n}=\frac{n!}{s^{n+1}}$, and case 4, $\laplace{e^{at}t^n}=\frac{n!}{(s-a)^{n+1}}$.
Case 5 and 7 share the same properties ($\cos{kt}$ and $e^{at}\cos{kt}$), and 6 and 8.

\begin{definition} [Unit Step Function]
	
	Defined as \\$U(t-a)$ or \\$H(t-a)=\begin{cases}
		0 & \text{if $0\leq t<a$}\\
		1 & \text{if $a \leq t$}

	\end{cases}$
\end{definition}

The step function is very useful for cases where some input can be toggled. We can use this to analyze differential equations with a toggle. 
These toggles can be added together as well, for example as $5U(t-2)-9U(t-8)$, with two switches toggled at the given times. A function like this can be 
written as $\begin{cases} 0 & 0 \leq t < 2 \\ 5 & 2 \leq t < 8 \\ -4 & 8 \leq t \end{cases}$.

In a generic case where the function toggles from a value of $f_1(t)$ to $f_2(t)$, it can be rewritten as $f(t)=f_1(t)+U(t-t_1)(f_2(t)-f_1(t))$. 
In general, a step function witn $n$ terms and $n-1$ different times can be written as $f_1+U(t-t_1)(f_2-f_1)+U(t-t_2)(f_3-f_2)+U(t-t_3)(f_4-f_3)...$

\subsubsection{Laplace transform of Heaviside function}

What is $\laplace[U(t-a)]$? Breaking up the integral into the terms from $0$ to $t_1$ and from $t_1$ to $\infty$, it becomes $\frac{e^{-as}}{s}$.


\begin{theorem}[Second Shifting Theorem]
	Let $\laplace[f(t)]=F(s)$.
	
	Then $\laplace[u(t-a)*f(t-a)]=e^{as}F(s)$
\end{theorem}

\begin{example} $\laplace[U(t-2)e^{3(t-2)}]$

	We see that the two $t-2$ terms match. We get $e^{-2s}$, and the func here is $f(t)=e^{3t}$, so answer is $\frac{e^{-2s}}{s-3}$.
\end{example}

\begin{example} $\laplace[U(t-7)\cos{4(t-7)}]$

	Again, $t-7$ match and $f(t)=\cos{4t}$ so we get $\frac{se^{-7t}}{s^2+16}$.
\end{example}

\begin{example} $\laplace[U(t-6)e^{2t}]$

	Harder here, we don't have $t-6$ in the exponent. If we force a $t-6$ in, we can add 12 to compensate and get $e^{2(t-6)}e^{12}$. We then get 
	\\$\laplace[U(t-6)e^{2(t-6)}e^{12}]=e^{12}\laplace[U(t-6)e^{2(t-6)}]=\frac{e^{12}e^{-6s}}{s-2}$
\end{example}

We can use this to create situations with toggling forces. For example, a 
spring mass system with $m=1,k=4$ and an external force of 4N at $t=3$
can be modeled as $y''+4y=4U(t-3)$.

\subsubsection{Inverse of second shifting theorem}

The operation is simple, $$\laplace^{-1}[e^{as}F(s)]=U(t-a)f(t-a)$$

\begin{example}
	$\laplace^{-1}[e^{-2s}\frac{3}{s^2-9}]=U(t-2)\sin{3(t-2)}$
\end{example}

\section{Chapter 4: Systems of Linear Equations}

\subsection{Basics of Matrices}
Given one linear equation (ex. $2x+3y=5$), how many x/y solutions are there? Since there's only one equation, there are infinite possible solutions. 
This can STILL BE IMPORTANT, but if you need one solution then this does not suffice.
Given 2 equations, how many possible solutions are there (ex. $x+y=5, 3x+4y=17$)? This one would have one solution, namely 3,2. There is, however, another 
situation with 2 solutions (ex. $x+y=5, 3x+3y=17$). This gives no possible solutions, and is known as an inconsistent system.

These linear equations can be stored in the form $a_{11}x_1+a_{12}x_2...a_{1n}x_n=b_1, a_{21}x_1+a_{22}x_2...a_{2n}x_n=b_2, ..., a_{m1}x_1+a_{m2}x_2...a_{mn}x_n=b_m$. 
This is a very unwieldy notation, but we can use matrices to represent these linear equations. We can insert the values as the rows/columns of a matrix, like so:
$$
\begin{bmatrix}
	a_{11} & a_{12} & a_{13} & ... & a_{1n} \\
	a_{21} & a_{22} & a_{23} & ... & a_{2n} \\
	a_{31} & a_{32} & a_{33} & ... & a_{3n} \\
	... & ... & ... & ... & ... \\
	a_{m1} & a_{m2} & a_{m3} & ... & a_{mn} \\
\end{bmatrix}
\begin{bmatrix}x_1 \\ x_2 \\ x_3 \\ .. \\ x_m\end{bmatrix} = \begin{bmatrix} b_1 \\ b_2 \\ b_3 \\ ... \\ b_m \end{bmatrix}
$$

This represents $A=m\times n$, using a matrix with $m$ rows and $n$ columns for the scalars and two $m$ length vectors, one for the unknowns and one for the
constants. This notation makes these notably easier to work with when it comes to calculations (especially with software). 

\subsubsection{Matrix operations}
Two matrices A and B can only be subtracted $\iff$ they have the same dimensions. The operation is simple, just add/subtract the corresponding entries of A and B.
\begin{example} $\begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} + \begin{bmatrix} 1 & 0 \\ 2 & 1 \end{bmatrix} = \begin{bmatrix} 2 & 2 \\ 5 & 5 \end{bmatrix}$
\end{example}

Multiplication is not as simple. First of all, matrices A and B can be multiplied to AB $\iff n_a=m_b$. The amount of rows of A or columns of B do not matter.
The resulting matrix is a matrix of size $m_a\times n_b$. This means that matrix multiplication is NOT commutative. The operation itself takes each row of A
and multiplies it with each column of B. For example,
$$\begin{bmatrix}a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix} * \begin{bmatrix}b_{11} & b_{12} & b_{13} \\ b_{21} & b_{22} & b_{23}\end{bmatrix}=$$
$$
\begin{bmatrix}
a_{11}b_{11}+a_{12}+b_{21} & a_{11}b_{11}+a_{12}+b_{22} & a_{11}b_{13}+a_{12}+b_{23} \\ 
a_{21}b_{11}+a_{22}+b_{21} & a_{21}b_{12}+a_{22}+b_{22} & a_{21}b_{13}+a_{22}+b_{23}
\end{bmatrix}$$

\begin{example} $\begin{bmatrix} 1 & 2 \\ 3 & 4 \\ 5 & 6 \end{bmatrix} * \begin{bmatrix} 2 \\ 5 \end{bmatrix}$
	
	Since sizes are $3\times 2, 2\times 1$, they result in a matrix of size $3\times 1$
	$$AB=\begin{bmatrix} 2+10 \\ 6 + 10 \\ 10 + 30\end{bmatrix}=\begin{bmatrix}12\\26\\70\end{bmatrix}$$
\end{example}

\begin{example} $\begin{bmatrix} 1&2\\3&4\\0&4\end{bmatrix} * \begin{bmatrix}1&0\\0&1\end{bmatrix}$
	$$AB=\begin{bmatrix} 1*1+2*0 & 1*0+2*1 \\ 3*1+0*4 & 3*0+1*4 \\ 0*1+0*4 & 0*0+1*4\end{bmatrix} = 
	\begin{bmatrix} 1 & 2 \\ 3 & 4 \\ 0 & 4 \end{bmatrix}$$
\end{example}

In the above example, we saw how the matrix $\begin{bmatrix}1&0\\0&1\end{bmatrix} $ gave the same value after multiplication. This is an example of the 
identity matrix, in this case for matrices of 2 columns. In general, the identity matrix follows the given pattern:
$$\begin{bmatrix}1&0&0&...&0\\0&1&0&...&0\\0&0&1&...&0\\...&...&...&...&...\\0&0&0&...&1\end{bmatrix}$$

Importantly, given a matrix A, $AI=IA=A$.

Other special matrices are:
\begin{itemize}
	\item A square matrix is a matrix where $m=n$.
	\item An upper triangular matrix is a square matrix where all the elements below the main diagonal are 0.
	\item A lower triangular matrix is a square matrix where all elements above the main diagonal are 0.
	\item A diagonal matrix is a square matrix where all the elements outside of the main diagonal are 0 (both lower and upper triangular).
	\item As seen before, an identity matrix is a diagonal matrix with all diagonal elements being 1.
\end{itemize}

The transpose of a matrix, $A^T$, is an $n\times m$ matrix with columns and rows swapped, and $A A^T, A^T A$ is always defined.

\subsection{Elementary Operations, Row Echelon, and Gaussian Elimination}

\begin{definition}
An $m\times n$ matrix A is said to be in Row Echelon Form if
\begin{enumerate}
	\item The first non-zero element of any row is a 1 (leading element).
	\item The leading one of any subsequent row is to the right of the 
		leading one of the row above
	\item Any rows full of 0 must be at the bottom.
\end{enumerate}
\end{definition}
Examples are $\begin{bmatrix}1&2&2\\0&1&3\\0&0&1\end{bmatrix}$ and 
$\begin{bmatrix}1&0\\0&1\\0&0\\0&0\end{bmatrix}$.

If a matrix is not an REF, we can make it an REF by using suitable Elementary Row Operations:
\begin{enumerate}
	\item Swap any two rows
	\item Multiply any row by a suitable non-zero scalar
	\item Add any two rows or their multiples
\end{enumerate}

\begin{example}
	Find REF of $\begin{bmatrix}1&-3&7&2\\2&-5&4&8\\-3&1&0&9\end{bmatrix}$

	We know we need a 0 at [1][0]. We can do $R2-R1$ and get $\begin{bmatrix}
	1&-3&7&2\\0&1&-10&4\\-3&1&0&9\end{bmatrix}$.

	We also need a 0 in R3, we can do $R3-3R1$: $\begin{bmatrix} 1&-3&7&2\\0&1&-10&4\\0&-8&21&15\end{bmatrix}$

	Replace R3 with R3+8R2:  $\begin{bmatrix} 1&-3&7&2\\ 0&1&-10&4\\0&0&-59&47\end{bmatrix}$
	
	Finally, divide R3 by -59: $\begin{bmatrix} 1&-3&7&2\\ 0&1&-10&4\\0&0&1&-\frac{47}{59}\end{bmatrix}$. 
\end{example}

Importantly, the REF is not always unique for a given matrix, there may be multiple different operations that can be done. 
The true unique form is the Reduced Row Echelon Form.
\begin{definition}
	The Reduced Row Echelon Form is an REF if the entries above each leading one are 0, and are unique.
\end{definition}

An example is $\begin{bmatrix}1&0&0&2&0\\0&1&0&3&0\\0&0&0&0&1\end{bmatrix}$. 
Both are usable. The REF leaves more work for later, while an RREF has more work up front.

The rank of a matrix is the number of nonzero rows of the matrix's REF or RREF.
\subsubsection{Gaussian Elimination}
\begin{definition}[Gaussian Elimination]
	Given a system of m linear equations and n variables $A\vec{x}=\vec{b}$, we define the augmented matrix
	$A|\vec{b}$ by joining B to A. To do Gaussian Elimination, $A|\vec{b}$ is converted to an REF and back substituted from the last row to the first.
\end{definition}

\begin{example}
	Solve $a+5b+2c=1, -a-4b+c=6, a+3b-3c=-9$.

	$A|B=\left[\begin{array}{ccc|c}
			1&5&2&-1\\-1&-4&1&6\\1&3&-3&-9
			\end{array}\right]$,
	after REF we get $\left[\begin{array}{ccc|c}
		1&5&2&1\\0&1&3&7\\0&0&1&4
	\end{array}\right]$

	Back substituting, we get\\ 
	$0a+0b+1c=4,c=4, 0a+1b+3*4=7, b=-5, a+5*-5+2*4=1=a=18$.\\
	This then gives
	the final solution $\vec{x}=\begin{bmatrix}18\\-5\\4\end{bmatrix}$.
\end{example}

Three outcomes can result after making the REF:
\begin{enumerate}
	\item All three rows have one item more than the previous and the
		last one isn't zero, giving one solution.
	\item The last matrix item is a row of 0, giving no solution
	\item There are more variables than functions, giving many solutions.
\end{enumerate}

In cases with more variables than functions, the matrix will result in some columns that do not end in a 1, called pivot columns. The variables corresponding
to pivot columns are called free variables, and the remaining variables
(basic variables) are solved for in terms of free variables.

\begin{example}
	Solve $a-2b+3c+2d+e=10, 2a-4b+8c+3d+10e=7, 3a-6b+10c+6d+5e=27$.

	Row echelon becomes 
	$\left[\begin{array}{ccccc|c}
			1&-2&3&2&1&10\\0&0&1&0&2&-3\\0&0&0&1&-4&7
	\end{array}\right]$

	Basic variables are $a,c,d$, free are $b,e$.

	$R3: d-4e=7, d=7+4e\\
	R2: c+2e=-3, c=2e-3\\
	R1: a-2b+3c+4d+e=10, a-2b+3(2e-3)+2(7+4e)+e=10, a-2b+6e-9+14+8e+e=10,\\
	a=5-3e+2b$.

	Solution becomes $\vec{x}=\begin{bmatrix}5-3e+2b\\0+0e+b\\-3-2e+0b\\
	7+4e+0b\\0+0e+0b\end{bmatrix}$

	In vector form, this is $\vec{x}=
		\begin{bmatrix} 5\\0\\-3\\7\\8\end{bmatrix}
		+e\begin{bmatrix}-3\\0\\-2\\4\\1\end{bmatrix}
		+b\begin{bmatrix}2\\1\\0\\0\\0\end{bmatrix}$
\end{example}

In all these cases, the right hand side $\vec{b} \neq \vec{0}$, called a non-homogeneous system.
If it is 0, it is a homogeneous system. Obviously, $\vec{x}=0$ is always a solution. This means that 
homogeneous systems are NEVER inconsistent (always 1 base solution). Is there another solution though? 

Since the only alternative to one solution is infinite solution, finding the answer is as easy as looking for free variables. Fortunately, we can skip the augmented column as its just 0.

These REFs can also be analyzed through their ranks. 
\begin{itemize}
	\item Rank=$m$, the system is consistent but nothing else is known.
	\item Rank=$n$, all columns are pivot columns, so no free variables
		and therefore there is only one solution.
\end{itemize}

\subsection{Inverse of a matrix}

Given A, an $n\times n$ square matrix, the inverse of A if it exists is
another $n\times n$ matrix denoted by $A^{-1}$ such that $AA^{-1}=A^{-1}A=I$.

\begin{itemize}
	\item 2x2 matrix 
		$A=\begin{bmatrix}a_{11}&a_{12}\\a_{21}&a_{22}\end{bmatrix}$: 
		
		The inverse is simple, 
		$$A^{-1}=\frac{1}{a_{11}a_{22}-a_{12}a_{21}}
		\begin{bmatrix}a_{22}&-a_{12}\\-a_{21}&a_{11}\end{bmatrix}$$

		The inverse only fails to exist when the denominator is 0.
	\item For 3x3 and bigger, use the Gauss Jordan method:
		\begin{enumerate}
			\item Augment A with $I_{n\times n}$
			\item Row reduce A to I, and then the augmented
				I becomes $A^{-1}$
		\end{enumerate}
\end{itemize}

How do we use the inverses?

Given a system $A\vec{x}=\vec{b}$, we can pre-multiply $A^{-1}$ on both sides:
$$A^{-1}A\vec{x}=A^{-1}\vec{b}, \vec{x}=A^{-1}\vec{b}$$

We can then just solve the right term to get $\vec{x}$.

\subsection{Determinants}

The determinant of a 2x2 matrix is calculated as $a_{11}a_{22}-a_{12}a_{21}$

Higher order elements have the following process for taking determinants:
\begin{enumerate}
	\item Finding the minor of each top row element $M_{ij}$
		(the $(n-1)\times(n-1)$ determinant obtained by deleting the 
		row and column containing $a_{ij}$). 
	\item Find each cofactor $C_{ij}=(-1)^{i+j}M_{ij}$ 
	\item Sum the cofactors
\end{enumerate}
This works with ANY row or column, not just the top one.

Determinant properties:
\begin{itemize}
	\item Swapping any two rows of a matrix, the determinant value negates.
	\item If any two rows or columns are either the same or a scalar 
		multiple of each other, the determinant is 0.
	\item The determinant of an upper or lower triangular matrix is
		the product of the numbers along the diagonal.
\end{itemize}

\begin{theorem}Invertible Matrix Theorem

	Given an $n\times n$ matrix A where $\det{A}\neq0$, the following are 
	true:
	\begin{itemize}
		\item A is invertible ($A^{-1}$ exists)
		\item The system $A\vec{x}=\vec{b}$ has a unique solution 
			$\vec{x}=A^{-1}\vec{b}$
		\item Rank(A)=n
		\item RREF(A)=$I_{n\times n}$
		\item System has no free variables
		\item The columns of A are linearly independent and form a basis for $\mathbb{R}^n$
	\end{itemize}
	If the determinant is 0, the inverses of all of the above statements
	are true.
\end{theorem}

\section{Vector Spaces}
\subsection{Vector}
The common definition of a vector is an element with magnitude and direction. 
This notion is only one way of making a vector, however. In general, a vector
can represent almost any mathematical object (matrices, polynomials, other 
functions, typical vectors, etc).

\begin{definition}
	A vector space (or vector set) V is a collection of homogeneous vectors
	with the following properties: 
	\begin{enumerate}
		\item If $\vec{u},\vec{v}$ are in V, then $\vec{u}+\vec{v}$ is
			also in V.
		\item If $k$ is a scalar and $\vec{u}$ is in V, then $k\vec{u}$
			is in V.
		\item $\vec{u}+\vec{v}=\vec{v}+vec{u}$
		\item $\vec{u}+(\vec{v}+\vec{w})=(\vec{u}+\vec{v})+\vec{w}$
		\item $k(\vec{u}+\vec{v})=k\vec{u}+k\vec{v}$
		\item $\vec{u}+\vec{0}=\vec{u}$
		\item $\vec{u}*1=\vec{u}$
	\end{enumerate}
\end{definition}

\begin{definition}
	Let V be  a vector space and S be a subset of V. We call S a subspace
	of V if 
	\begin{enumerate}
		\item For all $\vec{u}, \vec{v}$ in S, $\vec{u}+\vec{v}$ is 
			also in S.
		\item For a scalar k and $\vec{u}$ in S, $k\vec{u}$ is also
			in S.
	\end{enumerate}
\end{definition}

In general, solving these problems involves giving a bad example of the 
statement given.

\subsection{Nullspace of a matrix}
\begin{definition}
	For any $m\times n$ matrix A, the nullspace $Null(A)$ or $N(A)$ is the
	set of all $\vec{x}$ such that $A\vec{x}=\vec{0}$.
\end{definition}

This is equivalent to solving a homogeneous system.

\subsection{Span}

We can say that a vector $\vec{w}$ in in the span of $\{\vec{v}_1, \vec{v}_2, ... \vec{v}_n\}$ if we can find scalars $c_1,c_2,...c_n$
such that $c_1\vec{v}_1+c_2\vec{v}_2...c_n\vec{v}_n=\vec{w}$.

As an obvious example, given $\vec{v}_1=\begin{bmatrix}1\\1\\1 \end{bmatrix}, \vec{v}_2=\begin{bmatrix}2\\2\\2\end{bmatrix}$ and
$\vec{w}=\begin{bmatrix}4\\4\\4\end{bmatrix}$, it is in the span because $2\vec{v}_1+\vec{v}_2=\vec{w}$.

As an example, the reason that any vector is in the span of 3D space is because the base vectors $\hat{i}, \hat{j}, \hat{k}$ 
can be combined with any scalar to form any real triple of numbers.

This idea of span can be applied to any vector space, as the basic acts of scalar multiplication and vector addition are defined for 
any vector space and are all that is required.

To check whether a vector is in the span of another, you can create a linear system and check if the system is consistent. If it is 
consistent, the constants are easily solvable from the REF.

\subsection{Linearly Dependent Vectors}

\begin{definition}
	Given $\vec{v}_1,\vec{v}_2,...\vec{v}_n$, they are linearly dependent if we can find scalars $c_1,c_2,...c_n$ where not all 
	are zero and $c_1\vec{v}_1+c_2\vec{v}_2...c_n\vec{v}_n=\vec{0}$. If they are all equal to 0, the vectors are all linearly 
	independent.
\end{definition}

Some useful shortcuts:
\begin{enumerate}
	\item If $\vec{0}$ is in the list of vectors, the set is dependent.
	\item If we have $n$ vectors in $\mathbb{R}^n$ and if the determinant of the matrix with these vectors as columns is not 0,
		the vectors are independent.
	\item If we have $m$ vectors in $\mathbb{R}^n$ and $m>n$, the set is linearly dependent.
\end{enumerate}

Unfortunately, for $m<n$, there is NO shortcut, meaning that the only solution is setting up the homogeneous system and testing
for free variables. If there are free variables, the vectors are linearly dependent, otherwise they are independent.

How do we know if a set of functions are linearly dependent? If we try to do $c_1f_1+c_2f_2...c_nf_n=0$, we only have one equation, so
this is unsolveable. Fortunately, new equations here can be generated by doing $c_1f_1'+c_2f_2'+...c_nf_n'$, and taking derivatives
until we have enough values for a square matrix so a determinant can be taken. 

\subsection{Basis and Dimension}

\begin{definition} A set of vectors $\{\vec{v}_1, \vec{v}_2, ... \vec{v}_n\}$ is a basis for the vector space V if
	\begin{enumerate}
		\item The set spans V (Any vector in V can be written as a linear combination of the set)
		\item The set must be linearly independent
	\end{enumerate}
\end{definition}

An example of a basis would be the vectors $\hat{i}, \hat{j}, \hat{k}$ for the 3D vector space.

The amount of basis vectors for a given vector space V is the \textbf{dimension} of that vector space. For example, in the set of
all polynomials of degree 4 ($p(x)=a+bx+cx^2+dx^3+ex^4$), the basis is $\{1,x,x^2,x^3,x^4\}$ and the dimension is 5.

\begin{theorem}
	A set of $n$ linearly independent vectors in $\mathbb{R}^n$ will always form a basis for $\mathbb{R}^n$
\end{theorem}

\section{Eigenvalues and Eigenvectors}

$A=\begin{bmatrix}1&4\\2&3\end{bmatrix}, \vec{v}_1=\begin{bmatrix}-2\\1\end{bmatrix},\vec{v}_2=\begin{bmatrix}2\\3\end{bmatrix},
A\vec{v}_1=\begin{bmatrix}2\\-1\end{bmatrix}, A\vec{v}_2=\begin{bmatrix}14\\13\end{bmatrix}$

Why does $A\vec{v}_1$ result in the vector direction begin inverted but $A\vec{v}_2$ doesn't? Are there special vectors $\vec{v}$ for
which $A\vec{v}=$ a scalar multiple of $\vec{v}$?

These scalar multiples are called the eigenvalues, and their respective vectors they operate on are the eigenvectors.

\begin{definition}
	Let A be an $n\times n$ matrix. We can say that $\lambda$ is an eigenvalue of A if there exists a \emph{non-zero} vector 
	$\vec{v}$ such that $A\vec{v}=\lambda \vec{v}$. If this is true, $\vec{v}$ is the eigenvector of the eigenvalue $\lambda$.
\end{definition}

How do we find this?

For starters, we can rewrite $A\vec{v}=\lambda\vec{v}$ with $A\vec{v}=I_{n\times n}\lambda\vec{v}$. This can then be subtracted to
$(A-\lambda I)\vec{v}=0$. This is a homogeneous system so there are either free variables or there is no solution (because the trivial
solution $\vec{v}$ is not allowed); In other words, we're looking for the vector $\vec{v}$ as the basis of $A-\lambda I$.

Properties:
\begin{enumerate}
	\item Exactly n eigenvalues, with possible repetition.
		
		If all n are unique, the algebraic multiplicity is 1.
		
		If all n are repeated twice, the multiplicity is 2 (and so on).
	\item The sum of eigenvalues is always equal to the sum of the main diagonal.
	\item The product of the eigenvalues of A is equal to the determinant of A.
	\item The eigenvalue can be 0, +, -, imaginary, but its eigenvector can NEVER be 0.
\end{enumerate}

Solving these involves first solving $det(A-\lambda I)$ to get a polynomial of degree n, called the characteristic polynomial, and
using it to solve for the eigenvalues. Each eigenvalue is then used to solve the linear equation $(A-\lambda I)\vec{v}=0$.

\begin{example}
	$A=\begin{bmatrix}1&2\\3&4\end{bmatrix}$

	Do $\det(A-\lambda I)=0, \begin{bmatrix}1&2\\3&4\end{bmatrix}-\begin{bmatrix}\lambda&0\\0&\lambda\end{bmatrix}=
		\begin{bmatrix}1-\lambda&2\\3&4-\lambda\end{bmatrix}$

	$(1-\lambda)(3-\lambda)-8=0, 3-4\lambda+\lambda^2-8=0, \lambda=-1,5$

	Find nullspace for each:

		$\lambda_1, A-\lambda_1 I=\begin{bmatrix}2&4\\2&4\end{bmatrix}, x_2=u=free, x_1=-2um 
		\begin{bmatrix}x_1\\x_2\end{bmatrix}=u\begin{bmatrix}-2\\1\end{bmatrix}$

		Any value for u is valid here.
\end{example}

\subsection{Imaginary Eigenvalues}

Imaginary eigenvalues arise if the solutions for the characteristic polynomial are imaginary. Because of that, they always come in 
conjugate pairs.

\begin{theorem}
	If $\lambda_1=p+iq$ has an eigenvector $\vec{v_1}=u+iv$, then the second eigenvalue $\lambda_2=p-iq$ has an eigenvector 
	$\vec{v_2}=u-iv$
\end{theorem}

\begin{example}
	$A=\begin{bmatrix}2&5\\-5&2\end{bmatrix}, det(A-\lambda I)=(2-\lambda)^2+25, \lambda=2\pm5i$

	$\begin{bmatrix}2&5\\-5&2\end{bmatrix}-\begin{bmatrix}(2-5i)&0\\0&(2-5i)\end{bmatrix}=
		\begin{bmatrix}-5i&5i\\-5&-5i\end{bmatrix}\\
		R2-0.2R1=\begin{bmatrix}i&-1\\1&i\end{bmatrix}
		R2=R1, \begin{bmatrix}1&i\\i&-1\end{bmatrix}
		R2-iR1, \begin{bmatrix}1&i\\0&0\end{bmatrix}$

		$x_2=u=free, x_1=-iu, \vec{v_1}=\begin{bmatrix}-i&1\end{bmatrix}$
\end{example}

\subsection{Multiplicity}

The algebraic multiplicity $m_a$ represents the amount of repetitions, while the geometric multiplicity $m_g$ represents the amount of
free variables in $A-\lambda I$

For different matrix sizes, there are numerous different possibilities for multiplicities:
\begin{itemize}
	\item 2x2:
	\begin{enumerate}
		\item $\lambda_1\neq\lambda_2$,
			both $m_a$ and $m_g$ are 1
		\item $\lambda_1\neq\lambda_2$ and are imaginary, both $m_a$ and $m_g$ are 1
		\item $\lambda_1=\lambda_2$, $m_a=2$, $m_g=1$
	\end{enumerate}

\item 3x3:
	\begin{enumerate}
		\item $\lambda_1\neq\lambda_2\neq\lambda_3$, $m_a=1, m_g=1$
		\item $\lambda_1=\lambda_2\neq\lambda_3, m_a=2, m_g=1 OR m_g=2$
		\item $\lambda_1=\lambda_2=\lambda_3, m_a=3, m_g=1 OR m_g=2$
	\end{enumerate}
\end{itemize}

\section{Systems of linear first order DEs}

\end{document}
